#!/usr/bin/env python
"""
#######################################################################################################################
					affinitybinder_rnacompete2009_train.py
This scripts trains all the neural network present in the neuralbinder package on the data generated from the RNAcompete
experiments. More details about how the input file for this script was generated can be found in the documentation. The
input file required for this script can also be downloaded from the website directly.

Summary: Train deep learning models on RNAcompete_2009 datasets with
sequence + secondary structure profiles, i.e. paired-unpaired (pu) or structural
profiles (struct).
#######################################################################################################################
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os, sys, h5py
import numpy as np
import tensorflow as tf

from neuralbinder.neuralbindhelpers import helper
from neuralbinder.deepomics import neuralnetwork as nn
from neuralbinder.deepomics import utils, fit

#Ensuring whether the right number of arguments are being passed with the script
#set default args as -h , if no args:
if len(sys.argv) == 1:
    sys.argv[1:] = ["-h"]
    print("Wrong number of arguments entered")

parser = argparse.ArgumentParser(prog='affinitybinder_rnacompete2009_train')

parser.add_argument('-rbpname',action='store', dest = 'rbp_name', required=True, nargs='+',
                    help='Specify one of the RBPS name: Fusip, HuR, PTB, RBM4, SF2, SLM2, U1A, VTS1 or YB1. If you want to train all at once, type = \'all\' here')

parser.add_argument('-e', action='store', dest='epochnumber', type=int,
                    help='Number of epochs for training. Default = 200.', default=200)

parser.add_argument('-b', action='store', dest='batchsize', type=int,
                    help='Batch size used for training. Default = 100', default=100)

parser.add_argument('-m', action='store', dest='models',  nargs='+',
                    help='Neural network models that will be used for training. Currently there are three models supported by the package:\n 1. affinity_residualbind \n 2. affinity_conv_net \n 3.affinity_all_conv_net \n. Default = all three', default='all')

parser.add_argument('-n', action='store', dest='norm',
                    help='Type of data normalization method to use: \n 1. clip_norm (CLIP normalization) \n 2. log_norm (Log normalization) \n Default = log_norm', default='log_norm')

parser.add_argument('-d', action='store', dest='data', required=True,
                    help='Input RNAcompete data that will be used. Download the hdf5 from the website.')

parser.add_argument('-o', action='store', dest='output_dir', required=True,
                    help='Directory to store the models generated for each experiment.')

parser.add_argument('-ss', action='store', dest='secondary_structures', nargs='+',
                    help='Secondary structure to be used for the model.\n 1. only sequence (\'seq\')\n 2. Paired-Unpaired (\'pu\')\n 3.PHIME(\'struct\')\n Default:\'all\'')

inputs = parser.parse_args()

#Getting all the experiments from the dataset
all_experiments = ['Fusip', 'HuR', 'PTB', 'RBM4', 'SF2', 'SLM2', 'U1A', 'VTS1', 'YB1']
#Crosschecking if the specified RNACompete experiment is present or not
for exp in inputs.rbp_name:
    if exp not in all_experiments:
        raise NameError("Sorry "+ str(exp) + " is not found in the RNACompete experiment.")
        sys.exit(1)

# Getting the experiments to train on
if inputs.rbp_name[0] == "all":
    experiments = ['Fusip', 'HuR', 'PTB', 'RBM4', 'SF2', 'SLM2', 'U1A', 'VTS1', 'YB1']
elif:
    experiments = inputs.rbp_name

# Crosschecking the number of models specified
all_models = ['affinity_residualbind','affinity_conv_net','affinity_all_conv_net']

for model in inputs.models:
    if model not in all_models:
       raise NameError("Sorry " + str(model) + " currently not present in neuralbinder.")
       sys.exit(1)

models = input.models

# Getting the secondary structures required
all_ss = ['seq','pu','struct']
for ss in inputs.secondary_structures:
    if ss not in all_ss:
        raise NameError("Sorry, " + str(ss) + " is not recognized. Please type either - \'seq\', \'pu\' or \'struct\'.")
        sys.exit(1)
ss_types = input.secondary_structures


#Ensuring all the other parameters are set
batch_size = inputs.batchsize
num_epochs = inputs.epochnumber
data_path = inputs.data
results_path = helper.make_directory(inputs.output_dir)
normalize_method = inputs.norm

# loop over different secondary structure contexts
for ss_type in ss_types:
	print('input data: ' + ss_type)
	sstype_path = helper.make_directory(results_path, normalize_method+'_'+ss_type)

	# loop over different models
	for model in models:
		print('model: ' + model)
		model_path = helper.make_directory(sstype_path, model)

		# loop over different RNA binding proteins
		for rbp_name in experiments:
			print('Analyzing: '+ rbp_name)
			tf.reset_default_graph() # reset any tensorflow graphs
			np.random.seed(247) # for reproducibilitjy
			tf.set_random_seed(247) # for reproducibility

			# load rbp dataset
			train, valid, test = helper.load_dataset_hdf5(data_path, dataset_name=rbp_name, ss_type=ss_type)

			# process rbp dataset
			train, valid, test = helper.process_data(train, valid, test, method=normalize_method)

			# get shapes
			input_shape = list(train['inputs'].shape)
			input_shape[0] = None
			output_shape = train['targets'].shape

			# load model
			genome_model = helper.import_model(model)
			model_layers, optimization = genome_model(input_shape, output_shape)

			# build neural network class
			nnmodel = nn.NeuralNet(seed=247)
			nnmodel.build_layers(model_layers, optimization)
			nnmodel.inspect_layers()

			# compile neural trainer
			file_path = os.path.join(model_path, rbp_name)
			nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=file_path)

			# initialize session
			sess = utils.initialize_session(nnmodel.placeholders)

			# fit model
			data = {'train': train, 'valid': valid}
			fit.train_minibatch(sess, nntrainer, data, batch_size=batch_size, num_epochs=num_epochs,
				  patience=20, verbose=2, shuffle=True, save_all=False)

			sess.close()
